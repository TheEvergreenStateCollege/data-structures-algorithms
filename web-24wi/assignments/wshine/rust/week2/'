use reqwest::get;
use scraper::{Html, Selector};
use std::env;
async fn get_request(url: &str) -> Result<(), reqwest::Error> {
    let response = reqwest::get(url).await?;
    let status_code = response.status();
    let text = response.text().await?;
    let document = Html::parse_document(&text);
    let selector = Selector::parse("a").unwrap();
    //println!("{}", status_code);
    for e in document.tree {
        //println!("{:#?}", e);
    }
    //for link in document.select(&selector) {
    //    println!("{}", link
    //        .value()
    //        .attr("href")
    //        .expect("href not found")
    //    );
    //}

    Ok(())
}

struct Options {
    start_url: String,
    search_term: String,
    max_depth: u32,
}

#[tokio::main]
async fn main() {
    // args: start url, search term, max depth
    let options: Options;
    let args: Vec<String> = env::args().collect();
    match args.len() {
        4.. => {
            options = Options {
                start_url: args[1].to_string(),
                search_term: args[2].to_string(),
                max_depth: args[3].parse().expect("Not a valid number."),
            }
        }
        _ => panic!("Not enough arguments"),
    }
    println!("{ :#? }", options);
    let _ = get_request("https://www.w3schools.com/").await;
}
